{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[0,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "testLayer = ReLULayer()\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2537337939200146"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import TwoLayerNet\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "\n",
    "#Now implement regularizaion\n",
    "model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for hidden_W parameter\n",
      "Gradient check passed!\n",
      "Checking gradient for hidden_B parameter\n",
      "Gradient check passed!\n",
      "Checking gradient for output_W parameter\n",
      "Gradient check passed!\n",
      "Checking gradient for output_B parameter\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for hidden_W parameter\n",
      "Gradient check passed!\n",
      "Checking gradient for hidden_B parameter\n",
      "Gradient check passed!\n",
      "Checking gradient for output_W parameter\n",
      "Gradient check passed!\n",
      "Checking gradient for output_B parameter\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.329251, Train accuracy: 0.240556, val accuracy: 0.237000\n",
      "Epoch: 1, Loss: 5.333040, Train accuracy: 0.285222, val accuracy: 0.286000\n",
      "Epoch: 2, Loss: 4.935654, Train accuracy: 0.317778, val accuracy: 0.310000\n",
      "Epoch: 3, Loss: 5.105554, Train accuracy: 0.346556, val accuracy: 0.333000\n",
      "Epoch: 4, Loss: 4.659118, Train accuracy: 0.366667, val accuracy: 0.355000\n",
      "Epoch: 5, Loss: 4.924880, Train accuracy: 0.382333, val accuracy: 0.375000\n",
      "Epoch: 6, Loss: 4.635313, Train accuracy: 0.393222, val accuracy: 0.392000\n",
      "Epoch: 7, Loss: 4.838899, Train accuracy: 0.407889, val accuracy: 0.402000\n",
      "Epoch: 8, Loss: 4.586705, Train accuracy: 0.412444, val accuracy: 0.414000\n",
      "Epoch: 9, Loss: 4.005312, Train accuracy: 0.421000, val accuracy: 0.418000\n",
      "Epoch: 10, Loss: 4.103477, Train accuracy: 0.426000, val accuracy: 0.426000\n",
      "Epoch: 11, Loss: 4.391783, Train accuracy: 0.429889, val accuracy: 0.422000\n",
      "Epoch: 12, Loss: 4.141169, Train accuracy: 0.442667, val accuracy: 0.437000\n",
      "Epoch: 13, Loss: 4.015488, Train accuracy: 0.448444, val accuracy: 0.444000\n",
      "Epoch: 14, Loss: 3.863704, Train accuracy: 0.450222, val accuracy: 0.452000\n",
      "Epoch: 15, Loss: 3.854310, Train accuracy: 0.461556, val accuracy: 0.460000\n",
      "Epoch: 16, Loss: 3.975284, Train accuracy: 0.463667, val accuracy: 0.452000\n",
      "Epoch: 17, Loss: 3.771023, Train accuracy: 0.468667, val accuracy: 0.464000\n",
      "Epoch: 18, Loss: 3.717183, Train accuracy: 0.471667, val accuracy: 0.463000\n",
      "Epoch: 19, Loss: 3.754264, Train accuracy: 0.470667, val accuracy: 0.462000\n"
     ]
    }
   ],
   "source": [
    "from trainer import Trainer, Dataset\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(),num_epochs=20, learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc94a3a56a0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3iUdb7+8fcnjUAINREQCEWw0IQQAemIIiJ2XamigujasB3PunvOnl33d84e29pWcaliAcvaWHtFpAQIXUElQgidUBJCaAl8f39k2JMNCRnCZJ6Zyf26rlzM5Plm5r4eJjcP36eZcw4REQl/UV4HEBGRwFChi4hECBW6iEiEUKGLiEQIFbqISISI8eqNk5KSXMuWLb16exGRsLR06dJdzrnkspZ5VugtW7YkIyPDq7cXEQlLZraxvGWachERiRAqdBGRCKFCFxGJECp0EZEIoUIXEYkQKnQRkQihQhcRiRBhV+g5+Yf54z9+4EjRMa+jiIiElLAr9CVZe5g+P4vfvbcaXctdROT/hF2hD+nYhAkD2/L20s1M/PYXr+OIiIQMz079Px33XdyWDbsKePzTn2jZMIEhHZt4HUlExHNht4UOYGY8fn0nUlPqcf+bK1ixKdfrSCIingvLQgeIj41m8k1pnFGnBuNmZLAl96DXkUREPBW2hQ7QsHYNpo25gMOFRxn78hLyDxV6HUlExDNhXegAbRsl8uKoVNbt3M89s5ZTdFSHM4pI9RT2hQ7Qp20yj17Vnjk/5fD/PlrrdRwREU+E5VEuZRnZvQUbcgqYMm8DrZISGNOzpdeRRESCKmIKHeCRIeeRtfsAf/zHD6Q0qMWAc8/wOpKISNBExJTLcdFRxrPDOnNu4zrcM2s5P27f53UkEZGgiahCB0ioEcPUm9NIqBHN2Jcz2Jl/yOtIIiJBEXGFDtCkbk2mjrmAPQVHuO2VpRw8ctTrSCIiVc6vQjezLDNbbWYrzCyjjOUjzWyVb8wCMzs/8FFPTYemdXl2WGdWbc7lwbdXcOyYLuQlIpHtVLbQBzjnOjvn0spYtgHo55zrCPwJmBSQdKdpUPvG/Pay8/h49Xae+uInr+OIiFSpgBzl4pxbUOJpOtAsEK8bCOP6tGL9rv288M0vtGyYwA1pzb2OJCJSJfzdQnfA52a21MzGVzB2LPBJWQvMbLyZZZhZRk5OzqnkrDQz49GrOtCrTUN++95q0tfvDsr7iogEm7+F3ts5lwpcBtxlZn3LGmRmAygu9H8va7lzbpJzLs05l5acnFypwJURGx3FiyO7ktKgFre/upT1OfuD9t4iIsHiV6E757b4/twJvAd0Kz3GzDoBU4CrnHMhtxlct2Ys02/uRnSUMXZGBrv2H/Y6kohIQFVY6GaWYGaJxx8Dg4DvS41JAd4FRjvnfq6KoIGQ0rAWk0Z3ZVveQUZOXsRulbqIRBB/ttAbAfPMbCWwGPjIOfepmd1hZnf4xvweaAi8WN6hjaEirWUDpo25gKzdBYycsog9BUe8jiQiEhDm1Y2W09LSXEaGd70/b90uxs5YQqukBGbd1oP6CXGeZRER8ZeZLS3n8PHIPFPUH73bJjFlTBrrdxUwYsoi9mpLXUTCXLUtdCi+jvqUm9L4JWc/I6csIveASl1Ewle1LnSAvmcnM/mmNDJV6iIS5qp9oQP0OzuZv43uyrod+xk9dTF5B3RvUhEJPyp0nwHnnMHfRnflp+35jJ62iLyDKnURCS8q9BIGnHsGE0elsnbbPm6aqlIXkfCiQi9l4HmNmDiyK2u27eOmaYvZd0ilLiLhQYVehovbNeLFkV1ZszWPm6YuJl+lLiJhQIVejkvaNeKvI1L5fkseY6ap1EUk9KnQT+LS9o3564hUVm3O4+bpS9h/uMjrSCIi5VKhV2Bwh8Y8P7wLKzblcvO0xSp1EQlZKnQ/XNaxCc8P78LyTbncMn0xBSp1EQlBKnQ/DenYhOeGdWFZdi63TF+iUheRkKNCPwWXd2rCMzd2Zmn2Xh3SKCIhR4V+iq44/0yeH96FlZtyGT1lkS4TICIhQ4VeCUM6NuGlUV1Zuy2fEVPSdZMMEQkJKvRKurhdIybd1JXMnfsZPimdnHzdzk5EvOVXoZtZlpmtLu/2clbsOTPLNLNVZpYa+Kihp/85ZzD95gvI3nOAGyctZHveIa8jiUg1dipb6AOcc53LufXRZUBb39d4YGIgwoWDnm2SmHFrN3bkHeLGSQvZknvQ60giUk0FasrlKuAVVywdqGdmTQL02iGvW6sGvDquO3sKjnDj3xayac8BryOJSDXkb6E74HMzW2pm48tY3hTYVOL5Zt/3qo3UlPrMHNeD/YeL+NXfFrI+Z7/XkUSkmvG30Hs751Ipnlq5y8z6VubNzGy8mWWYWUZOTk5lXiKkdWxWl5njenCk6Bg3Tkpn3Y58ryOJSDXiV6E757b4/twJvAd0KzVkC9C8xPNmvu+Vfp1Jzrk051xacnJy5RKHuHZn1uGN8T0AGDYpnbXb9nmcSESqiwoL3cwSzCzx+GNgEPB9qWGzgZt8R7v0APKcc9sCnjZMtG2UyFu3X0hcTBTDJ6ezenOe15FEpBrwZwu9ETDPzFYCi4GPnHOfmtkdZnaHb8zHwHogE5gM3FklacNIq6QE3rr9QhLiYhgxJZ1l2Xu9jiQiEc6cc568cVpamsvIOOGQ9oizJfcgIyansyv/MNNv6Ua3Vg28jiQiYczMlpZz+LjOFK1qTevV5K3bL6Rx3XjGTFvMgsxdXkcSkQilQg+CRnXieWP8haQ0qMUtLy9hzk87vY4kIhFIhR4kyYk1mDW+B2cl12b8K0v5ZHW13WcsIlVEhR5EDRLimHVbDzo0rcNdM5cxc1G215FEJIKo0IOsbq1YXh/Xg35nJ/Pb91bz/Ffr8GrHtIhEFhW6B2rGRTPppjSu7dKUp774mT/+Yw3HjqnUReT0xHgdoLqKjY7iyRvOp0FCHFPmbWBPwRGevOF84mL0b6yIVI4K3UNRUcbvLj+PpMQa/O8nP7L3wBFeGtWVhBr6axGRU6fNQY+ZGXf0O4vHr+vE/MxdjJiySLe0E5FKUaGHiF9d0Nx3n9J9XP/SAt0oQ0ROmQo9hAxq35hXb+1GTv5hrp+4QJffFZFTokIPMd1bN+TN8RdSdMxxw98W6qJeIuI3FXoIandmHd65oyd1a8YycvIivtGlAkTEDyr0EJXSsBZ/v6MnrZISuG1GBu8vP+F+ISIi/0KFHsKSE2vwxu09SGtZn/veXMG0eRu8jiQiIUyFHuLqxMfy8i3dGNy+MY9+uIYnPvtRlwoQkTKp0MNAfGw0L4xMZXi3FF745hceeXc1RUePeR1LREKMTkkME9FRxv9c04Gk2nE8/3UmW3IP8tcRqdStGet1NBEJEX5voZtZtJktN7MPy1iWYmbf+JavMrMhgY0pUHxW6YODzuGx6zqSvn4317w4nw27CryOJSIh4lSmXCYAa8tZ9h/AW865LsAw4MXTDSblu/GCFF4b2529BUe4+oX5uq2diAB+FrqZNQMuB6aUM8QBdXyP6wJbTz+anEz31g354K7enJFYg9HTFvNa+kavI4mIx/zdQn8GeBgob0/cH4BRZrYZ+Bi4p6xBZjbezDLMLCMnJ+dUs0opKQ1r8e6dPenbNon/eP97/uuD77WzVKQaq7DQzWwosNM5t/Qkw4YDLzvnmgFDgFfN7ITXds5Ncs6lOefSkpOTKx1a/k9ifCxTxlzAbX1aMWPhRm55eQl5Bwq9jiUiHvBnC70XcKWZZQFvABeZ2WulxowF3gJwzi0E4oGkAOaUk4iOMn53eTsev67TP3eWrs/Z73UsEQmyCgvdOfeIc66Zc64lxTs8v3bOjSo1LBsYCGBm51Fc6JpTCbJfXdCc18f1IPdgIVe/MJ9567SzVKQ6qfSJRWb2qJld6Xv6IHCbma0EZgE3O53O6IlurRrwwV29aFw3njHTF/PqwiyvI4lIkJhXvZuWluYyMjI8ee/qIP9QIRPeWMHXP+5kdI8W/P6KdsRG68RgkXBnZkudc2llLdNveIRKjI9l8k1pjO/bmlfTN3Lz9MXaWSoS4VToESw6yvjtkPN44vpOLN6wh6tfnM8v2lkqErFU6NXADWnNmXlbD/IOFnLNC/P5bp32V4tEIhV6NXFBy+KdpU3q1uTm6UuYOOcXjh3TfmuRSKJCr0aaN6jFO3f2ZHD7xjz26Y+MnbGEPQVHvI4lIgGiQq9mateI4a8juvCnq9ozP3M3lz/3HRlZe7yOJSIBoEKvhsyM0Re25N07exIXE8WNk9J56VtNwYiEOxV6NdahaV3+cU9vBrdvzP9+oikYkXCnQq/m6sTHagpGJEKo0EVTMCIRQoUu/6QpGJHwpkKXf/HPKZirO2gKRiTMqNDlBGbG6B4tNAUjEmZU6FKuDk3r8qGmYETChgpdTipRUzAiYUOFLhUqawrm6S9+plA3pBYJKSp08dvxKZgrzz+TZ79ax7UvLmDdjnyvY4mIjwpdTklifCxP39iZl0alsiX3IJc/P4/Jc9dzVDtMRTznd6GbWbSZLTezD8tZ/iszW2NmP5jZzMBFlFA0uEMTPruvL/3OTua/P17L8MnpZO8+4HUskWrtVLbQJwBry1pgZm2BR4Bezrn2wH0ByCYhLjmxBpNGd+XJG85n7dZ9DH52LjMXZaP7g4t4w69CN7NmwOXAlHKG3Aa84JzbC+Cc2xmYeBLqzIzruzbj0/v70iWlHr99bzW3vLyEHfsOeR1NpNrxdwv9GeBhoLzDGs4Gzjaz+WaWbmaDyxpkZuPNLMPMMnJydBu0SNK0Xk1evbU7f7yyPenrdzPo6bl8sGKLttZFgqjCQjezocBO59zSkwyLAdoC/YHhwGQzq1d6kHNuknMuzTmXlpycXMnIEqqioowxPVvy8b19aJ2cwIQ3VnD3zOU6GUkkSPzZQu8FXGlmWcAbwEVm9lqpMZuB2c65QufcBuBnigteqqHWybV5+/YL+bdLz+HzNdsZ9PRcvlq7w+tYIhGvwkJ3zj3inGvmnGsJDAO+ds6NKjXsfYq3zjGzJIqnYNYHNqqEk5joKO4a0IYP7upNUu04xs7I4OG/ryT/UKHX0UQiVqWPQzezR83sSt/Tz4DdZrYG+Ab4N+fc7kAElPDW7sw6fHB3L+7sfxZ/X7qZwc98x4JfdnkdSyQimVc7rdLS0lxGRoYn7y3eWLpxLw+9vZINuwoY2T2Ff7/sXOrEx3odSySsmNlS51xaWct0pqgETdcW9fno3t6M7d2KWYuzufipb/lk9TYdCSMSICp0CapacTH859B2vH9XL5Jq1+DXry/jtleWsjX3oNfRRMKeCl080alZPWbf3YvfDjmXeZk5XPKXb3l5/gZdE0bkNKjQxTMx0VGM73sWX9zfj64tG/CHf6zh2okLWLttn9fRRMKSCl0817xBLWbccgHPDuvM5j0HuOL5eTz26Y8cKjzqdTSRsKJCl5BgZlzVuSlfPtCPa7o0ZeKcX7j0mbnMW6dDHEX8pUKXkFI/IY4nbjifmbd1J8qMUVMX8cBbK3T5ABE/qNAlJPU8K4lPJvThnovaMHvFVgY+NYd3l23WIY4iJ6FCl5AVHxvNg4PO4aN7+9AqKYEH3lrJ6KmL2bi7wOtoIiFJhS4h75zGifz9jp786eoOrNyUy6Cn5/LcV+u001SkFBW6hIWoKGN0jxZ88UA/Lm7XiL988TOXPP0tn/2wXdMwIj4qdAkrjevG88KIVGbe1p1asTHc/upSbpq2mMyd+72OJuI5FbqEpZ5nJfHRvb35ryvasWJTLoOfmcv/fLxWl+eVak2FLmErJjqKW3q14puH+nNdajMmf7eei576lneWbuaYLiEg1ZAKXcJeUu0aPHZ9J96/sxdn1qvJg2+v5PqXFvD9ljyvo4kElQpdIsb5zevx3q978sT1ncjec4Ar/jqPR95drZOSpNpQoUtEiYoybkhrztcP9efWXq14K2MTA56cwysLsyg6eszreCJVSoUuEalOfCz/ObQdn07oQ4emdfj9Bz8w9Pl5LFqvOyNK5PK70M0s2syWm9mHJxlznZk5Myvz9kgiwda2USKvje3OxJGp5B8q4sZJ6dwza7luqCER6VS20CcAa8tbaGaJvjGLTjeUSCCZGZd1bMKXD/RjwsC2fP7DdgY8OYenPv+JgsNFXscTCRi/Ct3MmgGXA1NOMuxPwGPAoQDkEgm4mnHR3H/J2Xz1YD8Gd2jM819n0v/JOby1ZJPulCQRwd8t9GeAh4Ey9yqZWSrQ3Dn30clexMzGm1mGmWXk5OScWlKRAGlWvxbPDuvCu3f2pHn9mjz8ziqGPj+PBZm69rqEtwoL3cyGAjudc0vLWR4F/AV4sKLXcs5Ncs6lOefSkpOTTzmsSCClptTnnV/35PnhXdh3sJARUxYxbkYG63N0GQEJT1bRhY3M7M/AaKAIiAfqAO8650b5ltcFfgGO/xY0BvYAVzrnMsp73bS0NJeRUe5ikaA6VHiU6fOzeOGbTA4VHmX0hS2YMLAt9WrFeR1N5F+Y2VLnXJkHnlRY6KVeqD/wkHNu6EnGzPGNOWlbq9AlFOXkH+bpL3/mjcXZJMbHcu/Atozu0YK4GB3hK6HhZIVe6U+pmT1qZldWPpZI6ElOrMH/XNORTyb0pVOzuvzpwzVc+sxcPtdleiUMnNIWeiBpC11CnXOOOT/n8N8frSVz534ubN2Q311+Hh2a1vU6mlRjVbKFLhLpzIwB55zBpxP68Ker2vPTjnyu+Os8/u3tlWzL04lJEnq0hS7ip7yDhbz4TSbT52eBwc09W/LrfmdRP0E7TiV4ArZTNJBU6BKuNu05wDNfruO95ZtJiIvhtr6tGdu7FQk1YryOJtWACl2kCvy8I58nP/uJz9fsIKl2HHcNaMOI7inUiIn2OppEMBW6SBVanr2Xxz/9iYXrd9O0Xk3uv+RsrunSlOgo8zqaRCDtFBWpQl1S6jPztu68OrYbDRLieOjtlQx+Zi6ffq9DHSW4VOgiAWBm9GmbzOy7ezFxZCpHneOO15Zy9YsLdI0YCRoVukgAHb9U7+f39eXx6zqRs+8QI6YsYtSURazclOt1PIlwmkMXqUKHCo/y+qJsXvgmkz0FR7isQ2MeHHQ2bc5I9DqahCntFBXxWP6hQqbO28CU7zZw4EgR16U2Y8LFbWlWv5bX0STMqNBFQsSegiO88E0mr6ZvxDnHiG4p3HVRG85IjPc6moQJFbpIiNmWd5DnvsrkrYxNxEVHcXOvltzet7Uu1ysVUqGLhKisXQU8/eXPzF65ldo1Yri9b2tu6aWzTqV8KnSREPfj9n089fnPfLFmBw0T4rhzQBtGdk8hPlZnncq/UqGLhInl2Xt58vOfmJ+5myZ145kwsC3XdW1GbLSOMJZiOlNUJEx0SanP6+N68Pq47jSqE89v3l3NoKfnMnvlVo4d01mncnIqdJEQ1KtNEu/d2ZPJN6URFx3FvbOWM+S57/hyzQ5dTkDK5Xehm1m0mS03sw/LWPaAma0xs1Vm9pWZtQhsTJHqx8y4pF0jPpnQh2eHdeZQ4VHGvZLBtRMXMD9zl4pdTnAqW+gTgLXlLFsOpDnnOgF/Bx4/3WAiUiwqyriqc1O+eKAff762I9vzDjFyyiJunJRO+vrdXseTEOJXoZtZM+ByYEpZy51z3zjnDviepgPNAhNPRI6LjY5ieLcUvnmoP3+8sj1ZuwoYNimdkVPSycja43U8CQH+bqE/AzwMHPNj7Fjgk0onEpGTio+NZkzPlsx9eAD/ObQdP23P5/qXFnLTtMUsz97rdTzxUIWFbmZDgZ3OuaV+jB0FpAFPlLN8vJllmFlGTk7OKYcVkf8THxvN2N6tmPvwAB657FxWb87lmhcXcOvLS1i9Oc/reOKBCo9DN7M/A6OBIiAeqAO865wbVWrcxcDzQD/n3M6K3ljHoYsE1v7DRcxYkMWkuevJO1jIJe0acf/FZ9PuzDpeR5MACtiJRWbWH3jIOTe01Pe7ULwzdLBzbp0/r6VCF6ka+YcKmT4/i8nfrSf/UBFDOjbmvovP5uxGumRvJKiSE4vM7FEzu9L39AmgNvC2ma0ws9mVfV0ROT2J8bHcO7At8/79Iu4d2Ja5P+/i0mfmcs+s5WTu3O91PKlCOvVfJMLlHjjC5O/WM31+FocKj3JV56bcc1EbWifX9jqaVIKu5SIi7N5/mElz1/PKwo0cLjrK0E5ncvdFbTQVE2ZU6CLyT7v2H2bKdxt4ZWEWBwuPclmHxtw9oK12noYJFbqInGBvwRGmzd/Ay/OzyD9cxMXnNeLegW3o1Kye19HkJFToIlKuvIOFvDw/i2nzN5B3sJD+5yRzz0Vt6dqivtfRpAwqdBGpUP6hQl5N38iU7zawp+AIvdo05J6L2tKjdUOvo0kJKnQR8duBI0XMXJTNS9+uZ9f+w3Rr2YB7B7alV5uGmJnX8ao9FbqInLJDhUd5Y3FxsW/fd4guKfW496K29D8nWcXuIRW6iFTa4aKjvJ2xmYlzfmFL7kE6Nq3LXQPOYlC7xkRFqdiDTYUuIqet8Ogx3lu2hRfmZLJx9wFaJydwR7+zuLpzU+JidPOzYFGhi0jAHD3m+OT7bUyc8ws/bN1H4zrxjOvTiuHdUkioEeN1vIinQheRgHPOMXfdLibOySR9/R7q1oxlTM+W3NyzJQ0S4ryOF7FU6CJSpZZl7+WlOb/w+Zod1IyNZli35ozr05qm9Wp6HS3iqNBFJCgyd+bz0rfreX/5FgCu6tyUO/q1pq2uFxMwKnQRCaotuQeZ+t0GZi3O5mDhUS5p14hf9z+L1BSdfXq6VOgi4ok9BUeYsSCLGQuzyD1QSI/WDfh1/zb0bZukY9krSYUuIp4qOFzEG0s2MeW79WzLO8S5jRO5tVcrrux8JvGx0V7HCysqdBEJCUeKjvHBii1MnbeBH7fn0zAhjpE9WjC6RwuSE2t4HS8sqNBFJKQ451i4fjfT5m3gqx93EhsVxRXnn8nY3q10XfYKnKzQ/T4LwMyigQxgSxk3ia4BvAJ0BXYDNzrnsiqdWEQimpnR86wkep6VxIZdBUyfv4G3MzbzzrLNXNi6Ibf2bsXAc8/QpQVOkd9b6Gb2AJAG1Cmj0O8EOjnn7jCzYcA1zrkbT/Z62kIXkZLyDhTyxpJsZizIYmveIVo2rMUtvVpxfddmOgO1hNOecjGzZsAM4L+BB8oo9M+APzjnFppZDLAdSHYneXEVuoiUpejoMT79YTtT521geXYuifExDO+WwpieLXWiEoGZcnkGeBgo7+yApsAmAOdckZnlAQ2BXaWCjAfGA6SkpPj51iJSncRERzG005kM7XQmy7L3Mm3eBqb6vga3b8ytvVvpbkrlqLDQzWwosNM5t9TM+p/OmznnJgGToHgL/XReS0QiX2pKfVJH1GdL7kFeWZjFrEXZfLR6G52b1+O2Pq25tH0jYqJ1pcfjKpxyMbM/A6OBIiAeqAO865wbVWKMplxEpMoVHC7inWWbmTpvAxt3H6BZ/Zrc0qsVN17QnNrVZJ49YIct+rbQHypjDv0uoGOJnaLXOud+dbLXUqGLSGUdPeb4cu0Opn63gcVZe0isEcPw7tVjnj0ghy2W8aKPAhnOudnAVOBVM8sE9gDDKvu6IiIViY4yLm3fmEvbN2blplymlJhnv7xjE8b1aUWnZvW8jhl0OrFIRCLC5r0HmLEgizcWbyL/cBHdWjZgXJ9WDDyvEdERdDy7zhQVkWoj/1Ahby7ZxPT5WWzJPUjLhrW4tXfx8ey14sJ/nl2FLiLVTtHRY3z2ww4mf7eeFZtyqVszlpG+efZGdeK9jldpKnQRqbaccyzL3svkuRv4bM12YqKMIR2bMLpHC7q2qB92l/Gtkp2iIiLhwMzo2qIBXUc3YOPuAqbPz+KdpZv5YMVWzm2cyKgeLbi6S9OIOOxRW+giUu0cOFLE7BVbeWXhRtZs20ftGjFcm9qUUT1acHaI3y5PUy4iImVwzrF8Uy6vpW/kw1XbOFJ0jG6tGjCqRwsGt29MXEzonYWqQhcRqcCegiO8nbGJ1xdlk73nAEm14xh2QQrDu6eE1MlKKnQRET8dO+aYuy6H19Kz+frHHQBcdG4jRvVIoW/bZM+v0a6doiIifoqKMvqfcwb9zzmDLbkHmbUomzeWZPPl2h20aFiLkd1TuL5rcxokxHkd9QTaQhcRqcCRouJrtL+WvpHFG/YQFx3FZR0bM6JbCt1aNQjqoY+achERCZCftucza3E27yzbTP6hItqcUZvh3VK4LrUp9WpV/Va7Cl1EJMAOHjnKh6u2MnNxNsuzc6kRE8XlHZswontKlZ6wpEIXEalCa7buY+bijby/fCv7DxdxTqNEhndrzjWpzahbMzag76VCFxEJgoLDRfxjZfFW+6rNecTHFt9Ob0T3FLo0rxeQrXYVuohIkH2/JY/XF2Uze8UWCo4c5dzGiYzsnsLVXZqSGF/5rXYVuoiIR/YfLuKDFVuYuSibH7buo2ZsNA8OOptxfVpX6vV0HLqIiEdq14hhZPcWjOiWwqrNecxclM2ZVXTmqQpdRCQIzIzzm9fj/OZVd2u8Cq88Y2bxZrbYzFaa2Q9m9scyxqSY2TdmttzMVpnZkKqJKyIi5fHnUmKHgYucc+cDnYHBZtaj1Jj/AN5yznWh+AbRLwY2poiIVKTCKRdXvNd0v+9prO+r9J5UB9TxPa4LbA1UQBER8Y9fF/s1s2gzWwHsBL5wzi0qNeQPwCgz2wx8DNxTzuuMN7MMM8vIyck5jdgiIlKaX4XunDvqnOsMNAO6mVmHUkOGAy8755oBQ4BXzeyE13bOTXLOpTnn0pKTk083u4iIlHBKt+NwzuUC3wCDSy0aC7zlG7MQiAeSAhFQRET8489RLslmVs/3uCZwCfBjqWHZwEDfmPMoLnTNqYiIBJE/x6E3AWaYWTTF/wC85Zz70MweBTKcc7OBB4HJZnY/xTtIb3ZenYIqIlJNeXbqv5nlABsr+eNJwK4Axgm0UM8HoZ9R+U6P8p2eUM7XwjlX5k5Izwr9dJhZRnnXMggFoZ4PQj+j8p0e5Ts9oZ6vPKe0U1REREKXCl1EJFzbo3UAAATlSURBVEKEa6FP8jpABUI9H4R+RuU7Pcp3ekI9X5nCcg5dREROFK5b6CIiUooKXUQkQoR0oZvZYDP7ycwyzew3ZSyvYWZv+pYvMrOWQczW3HcN+DW+68RPKGNMfzPLM7MVvq/fByuf7/2zzGy1771PuN+fFXvOt/5WmVlqELOdU2K9rDCzfWZ2X6kxQV9/ZjbNzHaa2fclvtfAzL4ws3W+P+uX87NjfGPWmdmYIOZ7wsx+9P0dvnf8zO4yfvakn4cqzPcHM9tS4u+xzPslVPT7XoX53iyRLct3IcKyfrbK199pc86F5BcQDfwCtAbigJVAu1Jj7gRe8j0eBrwZxHxNgFTf40Tg5zLy9Qc+9HAdZgFJJ1k+BPgEMKAHsMjDv+vtFJ8w4en6A/oCqcD3Jb73OPAb3+PfAI+V8XMNgPW+P+v7HtcPUr5BQIzv8WNl5fPn81CF+f4APOTHZ+Ckv+9Vla/U8qeA33u1/k73K5S30LsBmc659c65I8AbwFWlxlwFzPA9/jsw0MwsGOGcc9ucc8t8j/OBtUDTYLx3AF0FvOKKpQP1zKyJBzkGAr845yp75nDAOOfmAntKfbvk52wGcHUZP3opxZeW3uOc2wt8wYkXsauSfM65z51zRb6n6RRfFdUT5aw/f/jz+37aTpbP1x2/AmYF+n2DJZQLvSmwqcTzzZxYmP8c4/tA5wENg5KuBN9UTxeg9HXiAS604tv3fWJm7YMarPi6Op+b2VIzG1/Gcn/WcTAMo/xfIi/X33GNnHPbfI+3A43KGBMq6/JWiv/XVZaKPg9V6W7flNC0cqasQmH99QF2OOfWlbPcy/Xnl1Au9LBgZrWBd4D7nHP7Si1eRvE0wvnA88D7QY7X2zmXClwG3GVmfYP8/hUyszjgSuDtMhZ7vf5O4Ir/7x2Sx/qa2e+AIuD1coZ49XmYCJxF8S0st1E8rRGKhnPyrfOQ/30K5ULfAjQv8byZ73tljjGzGIpvf7c7KOmK3zOW4jJ/3Tn3bunlzrl9zrn9vscfA7FmFrTrxDvntvj+3Am8R/F/a0vyZx1XtcuAZc65HaUXeL3+SthxfCrK9+fOMsZ4ui7N7GZgKDDS94/OCfz4PFQJ59wOV3yTnGPA5HLe1+v1FwNcC7xZ3hiv1t+pCOVCXwK0NbNWvq24YcDsUmNmA8ePJrge+Lq8D3Og+ebbpgJrnXN/KWdM4+Nz+mbWjeL1HZR/cMwswcwSjz+meMfZ96WGzQZu8h3t0gPIKzG1ECzlbhV5uf5KKfk5GwN8UMaYz4BBZlbfN6UwyPe9Kmdmg4GHgSudcwfKGePP56Gq8pXcL3NNOe/rz+97VboY+NE5t7mshV6uv1Pi9V7Zk31RfBTGzxTv/f6d73uPUvzBheIbabwNZAKLgdZBzNab4v96rwJW+L6GAHcAd/jG3A38QPEe+3SgZxDztfa970pfhuPrr2Q+A17wrd/VQFqQ/34TKC7ouiW+5+n6o/gfl21AIcXzuGMp3i/zFbAO+BJo4BubBkwp8bO3+j6LmcAtQcyXSfH88/HP4fEjv84EPj7Z5yFI+V71fb5WUVzSTUrn8z0/4fc9GPl833/5+OeuxNigr7/T/dKp/yIiESKUp1xEROQUqNBFRCKECl1EJEKo0EVEIoQKXUQkQqjQRUQihApdRCRC/H//7z/JPyHgTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc94a3557b8>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 53.070348, Train accuracy: 0.195667, val accuracy: 0.205000\n",
      "Epoch: 1, Loss: 10.759354, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 2, Loss: 3.768708, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 3, Loss: 2.507835, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 4, Loss: 2.267025, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 5, Loss: 2.327494, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 6, Loss: 2.277025, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 7, Loss: 2.305110, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 8, Loss: 2.269907, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 9, Loss: 2.343462, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 10, Loss: 2.278349, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 11, Loss: 2.228865, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 12, Loss: 2.296438, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 13, Loss: 2.266228, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 14, Loss: 2.232561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 15, Loss: 2.266127, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 16, Loss: 2.275511, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 17, Loss: 2.316422, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 18, Loss: 2.162603, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 19, Loss: 2.356698, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.285799, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 1, Loss: 2.305556, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 2, Loss: 2.169268, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 3, Loss: 2.227925, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 4, Loss: 2.289553, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Epoch: 5, Loss: 2.289078, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 6, Loss: 2.236745, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 7, Loss: 2.312481, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 8, Loss: 2.269613, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 9, Loss: 2.225542, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 10, Loss: 2.308375, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 11, Loss: 2.304153, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 12, Loss: 2.293727, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 13, Loss: 2.256530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 14, Loss: 2.311210, Train accuracy: 0.168889, val accuracy: 0.170000\n",
      "Epoch: 15, Loss: 2.245606, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Epoch: 16, Loss: 2.301932, Train accuracy: 0.192889, val accuracy: 0.199000\n",
      "Epoch: 17, Loss: 2.319741, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 18, Loss: 2.321028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Epoch: 19, Loss: 2.255875, Train accuracy: 0.127778, val accuracy: 0.144000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 2, ..., 4, 2, 7], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 3, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dataset.train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.656823, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Epoch: 1, Loss: 2.516913, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Epoch: 2, Loss: 2.252700, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Epoch: 3, Loss: 2.010264, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Epoch: 4, Loss: 1.878531, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch: 5, Loss: 1.806800, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch: 6, Loss: 1.735520, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Epoch: 7, Loss: 1.649465, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 8, Loss: 1.558893, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 9, Loss: 1.474613, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 10, Loss: 1.416436, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 11, Loss: 1.382035, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 12, Loss: 1.353556, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 13, Loss: 1.320687, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 14, Loss: 1.283111, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 15, Loss: 1.249948, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 16, Loss: 1.223179, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 17, Loss: 1.203155, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 18, Loss: 1.188550, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 19, Loss: 1.175452, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 20, Loss: 1.162145, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 21, Loss: 1.149641, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 22, Loss: 1.139319, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 23, Loss: 1.130380, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 24, Loss: 1.122290, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 25, Loss: 1.116104, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 26, Loss: 1.110515, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 27, Loss: 1.105795, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 28, Loss: 1.101788, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 29, Loss: 1.098317, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 30, Loss: 1.095291, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 31, Loss: 1.092660, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 32, Loss: 1.090352, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 33, Loss: 1.088325, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 34, Loss: 1.086605, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 35, Loss: 1.085404, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 36, Loss: 1.084377, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 37, Loss: 1.083492, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 38, Loss: 1.082727, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 39, Loss: 1.082064, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 40, Loss: 1.081487, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 41, Loss: 1.080984, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 42, Loss: 1.080544, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 43, Loss: 1.080158, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 44, Loss: 1.079818, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 45, Loss: 1.079517, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 46, Loss: 1.079251, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 47, Loss: 1.079013, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 48, Loss: 1.078873, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Epoch: 49, Loss: 1.078619, Train accuracy: 0.533333, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-7)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=2.8e-2, num_epochs=50, batch_size=15)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 33.281668, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Epoch: 1, Loss: 32.254864, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Epoch: 2, Loss: 31.048355, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Epoch: 3, Loss: 29.601322, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch: 4, Loss: 27.853773, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Epoch: 5, Loss: 26.029445, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Epoch: 6, Loss: 24.034714, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Epoch: 7, Loss: 21.975017, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 8, Loss: 19.848111, Train accuracy: 0.733333, val accuracy: 0.200000\n",
      "Epoch: 9, Loss: 17.860928, Train accuracy: 0.733333, val accuracy: 0.200000\n",
      "Epoch: 10, Loss: 15.983105, Train accuracy: 0.733333, val accuracy: 0.200000\n",
      "Epoch: 11, Loss: 14.131378, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Epoch: 12, Loss: 12.402425, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 13, Loss: 10.792562, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 14, Loss: 9.306173, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 15, Loss: 7.964117, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 16, Loss: 6.751866, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 17, Loss: 5.679509, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 18, Loss: 4.746002, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Epoch: 19, Loss: 3.942637, Train accuracy: 0.733333, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-2)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=4e-1, num_epochs=20, batch_size=15)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.801769, Train accuracy: 0.262889, val accuracy: 0.264000\n",
      "Epoch: 1, Loss: 5.579680, Train accuracy: 0.326444, val accuracy: 0.312000\n",
      "Epoch: 2, Loss: 5.011766, Train accuracy: 0.377556, val accuracy: 0.381000\n",
      "Epoch: 3, Loss: 4.612413, Train accuracy: 0.430556, val accuracy: 0.415000\n",
      "Epoch: 4, Loss: 4.400833, Train accuracy: 0.481667, val accuracy: 0.463000\n",
      "Epoch: 5, Loss: 4.342537, Train accuracy: 0.521667, val accuracy: 0.508000\n",
      "Epoch: 6, Loss: 4.236347, Train accuracy: 0.566333, val accuracy: 0.566000\n",
      "Epoch: 7, Loss: 3.756008, Train accuracy: 0.592556, val accuracy: 0.577000\n",
      "Epoch: 8, Loss: 3.937789, Train accuracy: 0.605444, val accuracy: 0.601000\n",
      "Epoch: 9, Loss: 3.645066, Train accuracy: 0.625667, val accuracy: 0.605000\n",
      "Epoch: 10, Loss: 3.282560, Train accuracy: 0.632111, val accuracy: 0.627000\n",
      "Epoch: 11, Loss: 3.248752, Train accuracy: 0.644111, val accuracy: 0.635000\n",
      "Epoch: 12, Loss: 3.327053, Train accuracy: 0.652444, val accuracy: 0.628000\n",
      "Epoch: 13, Loss: 3.223341, Train accuracy: 0.648222, val accuracy: 0.633000\n",
      "Epoch: 14, Loss: 2.981717, Train accuracy: 0.661444, val accuracy: 0.633000\n",
      "Epoch: 15, Loss: 2.847025, Train accuracy: 0.651111, val accuracy: 0.626000\n",
      "Epoch: 16, Loss: 2.570987, Train accuracy: 0.675556, val accuracy: 0.631000\n",
      "Epoch: 17, Loss: 2.778924, Train accuracy: 0.670556, val accuracy: 0.636000\n",
      "Epoch: 18, Loss: 2.552175, Train accuracy: 0.679778, val accuracy: 0.626000\n",
      "Epoch: 19, Loss: 2.533746, Train accuracy: 0.674333, val accuracy: 0.638000\n",
      "Epoch: 20, Loss: 2.184094, Train accuracy: 0.694000, val accuracy: 0.645000\n",
      "Epoch: 21, Loss: 2.531039, Train accuracy: 0.691556, val accuracy: 0.642000\n",
      "Epoch: 22, Loss: 2.298597, Train accuracy: 0.687667, val accuracy: 0.643000\n",
      "Epoch: 23, Loss: 1.976968, Train accuracy: 0.706000, val accuracy: 0.663000\n",
      "Epoch: 24, Loss: 2.337003, Train accuracy: 0.694889, val accuracy: 0.648000\n",
      "Epoch: 25, Loss: 1.919365, Train accuracy: 0.696778, val accuracy: 0.655000\n",
      "Epoch: 26, Loss: 2.079676, Train accuracy: 0.695667, val accuracy: 0.638000\n",
      "Epoch: 27, Loss: 2.048493, Train accuracy: 0.710667, val accuracy: 0.658000\n",
      "Epoch: 28, Loss: 2.037291, Train accuracy: 0.707556, val accuracy: 0.650000\n",
      "Epoch: 29, Loss: 1.947716, Train accuracy: 0.713889, val accuracy: 0.652000\n",
      "Epoch: 30, Loss: 2.064407, Train accuracy: 0.719222, val accuracy: 0.654000\n",
      "Epoch: 31, Loss: 1.922403, Train accuracy: 0.721556, val accuracy: 0.656000\n",
      "Epoch: 32, Loss: 1.931028, Train accuracy: 0.721333, val accuracy: 0.654000\n",
      "Epoch: 33, Loss: 1.811801, Train accuracy: 0.722556, val accuracy: 0.668000\n",
      "Epoch: 34, Loss: 1.686697, Train accuracy: 0.727000, val accuracy: 0.662000\n",
      "Epoch: 35, Loss: 1.908920, Train accuracy: 0.727778, val accuracy: 0.665000\n",
      "Epoch: 36, Loss: 1.558890, Train accuracy: 0.715778, val accuracy: 0.653000\n",
      "Epoch: 37, Loss: 1.690082, Train accuracy: 0.732111, val accuracy: 0.654000\n",
      "Epoch: 38, Loss: 1.492209, Train accuracy: 0.731778, val accuracy: 0.660000\n",
      "Epoch: 39, Loss: 1.634160, Train accuracy: 0.723667, val accuracy: 0.657000\n",
      "Epoch: 40, Loss: 1.442879, Train accuracy: 0.734222, val accuracy: 0.662000\n",
      "Epoch: 41, Loss: 1.472247, Train accuracy: 0.741889, val accuracy: 0.662000\n",
      "Epoch: 42, Loss: 1.361825, Train accuracy: 0.739444, val accuracy: 0.659000\n",
      "Epoch: 43, Loss: 1.570906, Train accuracy: 0.734111, val accuracy: 0.662000\n",
      "Epoch: 44, Loss: 1.708255, Train accuracy: 0.730778, val accuracy: 0.644000\n",
      "Epoch: 45, Loss: 1.561389, Train accuracy: 0.740333, val accuracy: 0.658000\n",
      "Epoch: 46, Loss: 1.408902, Train accuracy: 0.737889, val accuracy: 0.653000\n",
      "Epoch: 47, Loss: 1.559723, Train accuracy: 0.738111, val accuracy: 0.662000\n",
      "Epoch: 48, Loss: 1.570523, Train accuracy: 0.747778, val accuracy: 0.669000\n",
      "Epoch: 49, Loss: 1.308769, Train accuracy: 0.740444, val accuracy: 0.669000\n",
      "Epoch: 50, Loss: 1.286358, Train accuracy: 0.750778, val accuracy: 0.665000\n",
      "Epoch: 51, Loss: 1.380453, Train accuracy: 0.752000, val accuracy: 0.672000\n",
      "Epoch: 52, Loss: 1.024026, Train accuracy: 0.750222, val accuracy: 0.660000\n",
      "Epoch: 53, Loss: 1.007711, Train accuracy: 0.763778, val accuracy: 0.685000\n",
      "Epoch: 54, Loss: 1.407341, Train accuracy: 0.771667, val accuracy: 0.681000\n",
      "Epoch: 55, Loss: 1.645406, Train accuracy: 0.777556, val accuracy: 0.683000\n",
      "Epoch: 56, Loss: 1.307444, Train accuracy: 0.791222, val accuracy: 0.687000\n",
      "Epoch: 57, Loss: 1.451140, Train accuracy: 0.798333, val accuracy: 0.688000\n",
      "Epoch: 58, Loss: 1.275795, Train accuracy: 0.800000, val accuracy: 0.694000\n",
      "Epoch: 59, Loss: 1.218461, Train accuracy: 0.796111, val accuracy: 0.689000\n",
      "Epoch: 60, Loss: 1.271429, Train accuracy: 0.796333, val accuracy: 0.692000\n",
      "Epoch: 61, Loss: 1.361409, Train accuracy: 0.799444, val accuracy: 0.695000\n",
      "Epoch: 62, Loss: 1.466923, Train accuracy: 0.803000, val accuracy: 0.694000\n",
      "Epoch: 63, Loss: 1.031787, Train accuracy: 0.810444, val accuracy: 0.717000\n",
      "Epoch: 64, Loss: 1.273288, Train accuracy: 0.821222, val accuracy: 0.704000\n",
      "Epoch: 65, Loss: 1.203727, Train accuracy: 0.834333, val accuracy: 0.714000\n",
      "Epoch: 66, Loss: 0.947859, Train accuracy: 0.829000, val accuracy: 0.714000\n",
      "Epoch: 67, Loss: 1.027039, Train accuracy: 0.829333, val accuracy: 0.716000\n",
      "Epoch: 68, Loss: 1.241018, Train accuracy: 0.839000, val accuracy: 0.719000\n",
      "Epoch: 69, Loss: 1.161226, Train accuracy: 0.840111, val accuracy: 0.737000\n",
      "Epoch: 70, Loss: 0.945485, Train accuracy: 0.842778, val accuracy: 0.721000\n",
      "Epoch: 71, Loss: 0.917184, Train accuracy: 0.840444, val accuracy: 0.728000\n",
      "Epoch: 72, Loss: 0.993484, Train accuracy: 0.829889, val accuracy: 0.724000\n",
      "Epoch: 73, Loss: 1.300430, Train accuracy: 0.845000, val accuracy: 0.734000\n",
      "Epoch: 74, Loss: 0.756390, Train accuracy: 0.846333, val accuracy: 0.742000\n",
      "Epoch: 75, Loss: 0.944941, Train accuracy: 0.850333, val accuracy: 0.736000\n",
      "Epoch: 76, Loss: 0.915832, Train accuracy: 0.846444, val accuracy: 0.735000\n",
      "Epoch: 77, Loss: 0.946339, Train accuracy: 0.853222, val accuracy: 0.733000\n",
      "Epoch: 78, Loss: 0.978552, Train accuracy: 0.836111, val accuracy: 0.725000\n",
      "Epoch: 79, Loss: 1.142221, Train accuracy: 0.852333, val accuracy: 0.732000\n",
      "Epoch: 80, Loss: 1.011750, Train accuracy: 0.849556, val accuracy: 0.730000\n",
      "Epoch: 81, Loss: 1.097094, Train accuracy: 0.850000, val accuracy: 0.733000\n",
      "Epoch: 82, Loss: 1.129570, Train accuracy: 0.846778, val accuracy: 0.726000\n",
      "Epoch: 83, Loss: 0.968226, Train accuracy: 0.847889, val accuracy: 0.725000\n",
      "Epoch: 84, Loss: 0.746598, Train accuracy: 0.853222, val accuracy: 0.736000\n",
      "Epoch: 85, Loss: 1.023225, Train accuracy: 0.857111, val accuracy: 0.745000\n",
      "Epoch: 86, Loss: 0.846272, Train accuracy: 0.862222, val accuracy: 0.736000\n",
      "Epoch: 87, Loss: 0.941672, Train accuracy: 0.865222, val accuracy: 0.737000\n",
      "Epoch: 88, Loss: 1.044353, Train accuracy: 0.870333, val accuracy: 0.746000\n",
      "Epoch: 89, Loss: 0.850877, Train accuracy: 0.889111, val accuracy: 0.762000\n",
      "Epoch: 90, Loss: 0.996776, Train accuracy: 0.909111, val accuracy: 0.777000\n",
      "Epoch: 91, Loss: 0.864421, Train accuracy: 0.911778, val accuracy: 0.781000\n",
      "Epoch: 92, Loss: 1.025878, Train accuracy: 0.903222, val accuracy: 0.777000\n",
      "Epoch: 93, Loss: 0.893587, Train accuracy: 0.916111, val accuracy: 0.779000\n",
      "Epoch: 94, Loss: 0.719498, Train accuracy: 0.913444, val accuracy: 0.780000\n",
      "Epoch: 95, Loss: 0.753119, Train accuracy: 0.912000, val accuracy: 0.783000\n",
      "Epoch: 96, Loss: 0.729391, Train accuracy: 0.913556, val accuracy: 0.777000\n",
      "Epoch: 97, Loss: 0.835590, Train accuracy: 0.912778, val accuracy: 0.776000\n",
      "Epoch: 98, Loss: 0.622279, Train accuracy: 0.918778, val accuracy: 0.782000\n",
      "Epoch: 99, Loss: 0.849537, Train accuracy: 0.918222, val accuracy: 0.783000\n"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rate = 1e-2\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.995\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, reg = reg_strength)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, \n",
    "                  dataset, MomentumSGD(), \n",
    "                  learning_rate=learning_rate,\n",
    "                  num_epochs=num_epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  learning_rate_decay=learning_rate_decay\n",
    "                 )\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = model\n",
    "best_loss_history = loss_history\n",
    "best_train_history = train_history\n",
    "best_val_history = val_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(best_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.744000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse_ai_venv",
   "language": "python",
   "name": "dlcourse_ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
